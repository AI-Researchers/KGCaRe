# Think-On-Graph on HotpotQA dataset

This repository includes the code for paper "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph" The link to the original repo is [here](https://github.com/IDEA-FinAI/ToG).


## Project structure
* `data/` -- This folder contains all datasets used in the paper. See `data/README.md` for details.
* `eval/` -- This folder contains evaluation scripts.
* `ToG/` -- This folder contains source codes for Think on Graph approach. See `ToG/README.md` for details.
    - `output`: Contains the output files generated by the code.
    - `kg_utils_hotpotqa.py`: All the functions for querying KG used in `main_think_on_graph_hotpotqa.py`.
    - `main_think_on_graph_hotpotqa.py`: The main ToG algorithm using HotpotQA dataset.
    - `prompt_list_hotpotqa.py`: The prompts for LLMs to pruning, reasoning and generating.
    - `utils_hotpotqa.py`: All the functions used in ToG with HotpotQA dataset. 
* `requirements.txt` -- Pip environment file.

## Requirement installation
```
conda create --name tog_env python=3.9
conda activate tog_env
pip install -r requirements.txt
```
Make sure you have OpenAI API subscription to run any OpenAI models.

## Get started
We use SPARQL data end point to access Wikidata KG to run ToG on HotpotQA dataset.

## Running the experiments 

Navigate to the `ToG/` folder and then run below command:
```sh
python main_think_on_graph_hotpotqa.py \  
--dataset hotpotqa500 \ # dataset your wanna test, see ToG/data/README.md
--max_length 256 \ 
--temperature_exploration 0.1 \ # the temperature in exploration stage.
--temperature_reasoning 0.1 \ # the temperature in reasoning stage.
--width 3 \ # choose the search width of ToG, 3 is the default setting.
--depth 3 \ # choose the search depth of ToG, 3 is the default setting.
--remove_unnecessary_rel True \ # whether removing unnecessary relations.
--LLM_type gpt-3.5-turbo \ # the LLM you choose
--opeani_api_keys sk-xxxx \ # your own api keys, if LLM_type == llama, this parameter would be rendered ineffective.
--num_retain_entity 5 \ # Number of entities retained during entities search.
--prune_tools llm \ # prune tools for ToG, can be llm (same as LLM_type).
--output_file ./output/ToG_output.jsonl \ # Name of output file
```
For example:
```sh
python main_think_on_graph_hotpotqa.py --dataset hotpotqa500 --LLM_type gpt-3.5-turbo --opeani_api_keys <ADD_OPENAI_API_KEY>
```

LLM Models used: 
* Mixtral-8X7B-Instruct-v0.1 model
* Mistral-7B-Instruct-v0.2 model model
* GPT 3.5 Turbo
* GPT 4o

We use vLLM for Mistral and Mixtral models, so make sure you initialise the required vllm server:

For example:
```
python -m vllm.entrypoints.openai.api_server --model mistralai/Mixtral-8X7B-Instruct-v0.1 --tensor-parallel-size 4 --api-key token-abc123 
```

## Evaluation
The results of the predictions are evaluated against the reference answers using metrics like:

* EM (Exact Match): Measures if the predicted answer is exactly the same as the reference.
* F1 Score: Measures the overlap between the predicted and reference answers.

To run the evaluation script, go to `eval/` folder in a terminal and run below code:
```
bash evaluate_all.sh ./prediction_results ../data/stratified_hotpotqa_500sample_with_tag.json
```